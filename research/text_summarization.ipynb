{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install transformers[sentencepiece] datasets sacrebleu rouge_score py7zr -q\n",
    "!pip install transformers==4.37.2\n",
    "!pip install datasets==2.17.0\n",
    "!pip install evaluate==0.4.1\n",
    "!pip install rouge-score==0.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import load_dataset, load_from_disk, load_metric\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline, set_seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import huggingface_hub\n",
    "\n",
    "huggingface_hub.login(token=\"TOKEN_API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model_ckpt = \"facebook/bart-large-xsum\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Define the URL of the ZIP file\n",
    "url = 'https://github.com/entbappy/Branching-tutorial/raw/master/summarizer-data.zip'\n",
    "\n",
    "# Define the file paths\n",
    "zip_file_path = 'summarizer-data.zip'\n",
    "extracted_folder_path = 'summarizer-data'\n",
    "\n",
    "# Download the ZIP file\n",
    "response = requests.get(url)\n",
    "with open(zip_file_path, 'wb') as zip_file:\n",
    "    zip_file.write(response.content)\n",
    "\n",
    "# Extract the contents of the ZIP file\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extracted_folder_path)\n",
    "\n",
    "# Remove the ZIP file\n",
    "os.remove(zip_file_path)\n",
    "\n",
    "print('Downloaded and extracted the ZIP file successfully.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk(\"summarizer-data/samsum_dataset/\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate split lengths\n",
    "split_lengths = [len(dataset[split]) for split in dataset]\n",
    "\n",
    "\n",
    "print(\"Split lengths:\", split_lengths)\n",
    "\n",
    "\n",
    "print(\"Features:\", dataset['train'].column_names)\n",
    "\n",
    "\n",
    "print(\"\\nDialogue:\")\n",
    "print(dataset[\"test\"][1][\"dialogue\"])\n",
    "\n",
    "\n",
    "print(\"\\nSummary:\")\n",
    "print(dataset[\"test\"][1][\"summary\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(dataset[\"train\"])\n",
    "test_df = pd.DataFrame(dataset[\"test\"])\n",
    "validation_df = pd.DataFrame(dataset[\"validation\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Assuming train, test, and val are pandas DataFrames defined elsewhere in your code\n",
    "\n",
    "# Convert pandas DataFrames to Dataset objects\n",
    "train_ds = Dataset.from_pandas(train_df)\n",
    "test_ds = Dataset.from_pandas(test_df)\n",
    "val_ds = Dataset.from_pandas(validation_df)\n",
    "\n",
    "print(f\"Train Dataset:\\n{train_ds}\\n\\n\")\n",
    "print(f\"Test Dataset:\\n{test_ds}\\n\\n\")\n",
    "print(f\"Validation Dataset:\\n{val_ds}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_example(example):\n",
    "    \"\"\"\n",
    "    Preprocesses a single example (data point) for BART model input.\n",
    "\n",
    "    Args:\n",
    "        example: A dictionary containing 'dialogue' (list of text) and 'summary' (text).\n",
    "\n",
    "    Returns:\n",
    "        model_inputs: A dictionary of tokenized inputs and labels ready for BART. \n",
    "    \"\"\"\n",
    "\n",
    "    # Extract dialogues and prepare for tokenization\n",
    "    inputs = example['dialogue']\n",
    "\n",
    "    # Tokenize input dialogues for BART\n",
    "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
    "\n",
    "    # Tokenize target summaries\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(example['summary'], max_length=128, truncation=True)\n",
    "\n",
    "    # Include tokenized labels in model inputs\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "# Tokenize and preprocess datasets\n",
    "tokenized_train = train_ds.map(preprocess_example, batched=True, remove_columns=['id', 'dialogue', 'summary'])\n",
    "tokenized_test = test_ds.map(preprocess_example, batched=True, remove_columns=['id', 'dialogue', 'summary'])\n",
    "tokenized_val = val_ds.map(preprocess_example, batched=True, remove_columns=['id', 'dialogue', 'summary'])\n",
    "\n",
    "print(tokenized_train)\n",
    "print(tokenized_test)\n",
    "print(tokenized_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking samples\n",
    "sample = tokenized_train[0]\n",
    "print(\"Input IDs:\")\n",
    "print(sample['input_ids'])\n",
    "print(\"\\nAttention Mask:\")\n",
    "print(sample['attention_mask'])\n",
    "print(\"\\nLabels:\")\n",
    "print(sample['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
