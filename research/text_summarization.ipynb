{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install transformers[sentencepiece] datasets sacrebleu rouge_score py7zr -q\n",
    "!pip install transformers==4.37.2\n",
    "!pip install datasets==2.17.0\n",
    "!pip install evaluate==0.4.1\n",
    "!pip install rouge-score==0.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import torch\n",
    "from datasets import Dataset, load_dataset, load_from_disk, load_metric\n",
    "from huggingface_hub import huggingface_hub\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, BartForConditionalGeneration, DataCollatorForSeq2Seq,Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')  # Downloading necessary NLTK data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "huggingface_hub.login(token=\"TOKEN_API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_ckpt = \"facebook/bart-large-xsum\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URL of the ZIP file\n",
    "url = 'https://github.com/entbappy/Branching-tutorial/raw/master/summarizer-data.zip'\n",
    "\n",
    "# Define the file paths\n",
    "zip_file_path = 'summarizer-data.zip'\n",
    "extracted_folder_path = 'summarizer-data'\n",
    "\n",
    "# Download the ZIP file\n",
    "response = requests.get(url)\n",
    "with open(zip_file_path, 'wb') as zip_file:\n",
    "    zip_file.write(response.content)\n",
    "\n",
    "# Extract the contents of the ZIP file\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extracted_folder_path)\n",
    "\n",
    "# Remove the ZIP file\n",
    "os.remove(zip_file_path)\n",
    "\n",
    "print('Downloaded and extracted the ZIP file successfully.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk(\"summarizer-data/samsum_dataset/\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate split lengths\n",
    "split_lengths = [len(dataset[split]) for split in dataset]\n",
    "\n",
    "\n",
    "print(\"Split lengths:\", split_lengths)\n",
    "print(\"Features:\", dataset['train'].column_names)\n",
    "print(\"\\nDialogue:\")\n",
    "print(dataset[\"test\"][1][\"dialogue\"])\n",
    "print(\"\\nSummary:\")\n",
    "print(dataset[\"test\"][1][\"summary\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(dataset[\"train\"])\n",
    "test_df = pd.DataFrame(dataset[\"test\"])\n",
    "validation_df = pd.DataFrame(dataset[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming train, test, and val are pandas DataFrames defined elsewhere in your code\n",
    "# Convert pandas DataFrames to Dataset objects\n",
    "train_ds = Dataset.from_pandas(train_df)\n",
    "test_ds = Dataset.from_pandas(test_df)\n",
    "val_ds = Dataset.from_pandas(validation_df)\n",
    "\n",
    "print(f\"Train Dataset:\\n{train_ds}\\n\\n\")\n",
    "print(f\"Test Dataset:\\n{test_ds}\\n\\n\")\n",
    "print(f\"Validation Dataset:\\n{val_ds}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_example(example):\n",
    "    \"\"\"\n",
    "    Preprocesses a single example (data point) for BART model input.\n",
    "\n",
    "    Args:\n",
    "        example: A dictionary containing 'dialogue' (list of text) and 'summary' (text).\n",
    "\n",
    "    Returns:\n",
    "        model_inputs: A dictionary of tokenized inputs and labels ready for BART. \n",
    "    \"\"\"\n",
    "\n",
    "    # Extract dialogues and prepare for tokenization\n",
    "    inputs = example['dialogue']\n",
    "\n",
    "    # Tokenize input dialogues for BART\n",
    "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
    "\n",
    "    # Tokenize target summaries\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(example['summary'], max_length=128, truncation=True)\n",
    "\n",
    "    # Include tokenized labels in model inputs\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "# Tokenize and preprocess datasets\n",
    "tokenized_train = train_ds.map(preprocess_example, batched=True, remove_columns=['id', 'dialogue', 'summary'])\n",
    "tokenized_test = test_ds.map(preprocess_example, batched=True, remove_columns=['id', 'dialogue', 'summary'])\n",
    "tokenized_val = val_ds.map(preprocess_example, batched=True, remove_columns=['id', 'dialogue', 'summary'])\n",
    "\n",
    "print(tokenized_train)\n",
    "print(tokenized_test)\n",
    "print(tokenized_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking samples\n",
    "sample = tokenized_train[0]\n",
    "print(\"Input IDs:\")\n",
    "print(sample['input_ids'])\n",
    "print(\"\\nAttention Mask:\")\n",
    "print(sample['attention_mask'])\n",
    "print(\"\\nLabels:\")\n",
    "print(sample['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BartForConditionalGeneration.from_pretrained(model_ckpt).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Evaluation Metric\n",
    "\n",
    "### What is ROUGE?\n",
    "\n",
    "ROUGE (ROUGE-L, ROUGE-N, ROUGE-W) is a widely used metric for evaluating the quality of machine-generated text, particularly in the context of text summarization and machine translation. It measures how well a generated text (candidate) matches a reference text (ground truth) by comparing their common n-grams (sequence of n consecutive characters).\n",
    "\n",
    "### How does ROUGE work?\n",
    "\n",
    "ROUGE calculates three main scores:\n",
    "\n",
    "- **ROUGE-L:** Focuses on matching long n-grams (typically n = 1 or 2). This is useful for evaluating fluency and coherence of the generated text.\n",
    "\n",
    "- **ROUGE-N:** Evaluates matches of n-grams of any length (typically n = 1, 2, 3, 4, 5, 6). This provides a more comprehensive assessment of the generated text's similarity to the reference.\n",
    "\n",
    "- **ROUGE-W:** Considers n-grams weighted by their frequency in the reference text. This emphasizes the importance of matching more common phrases.\n",
    "\n",
    "### ROUGE Scores\n",
    "\n",
    "Each ROUGE score is calculated as a combination of precision and recall:\n",
    "\n",
    "- **Precision:** The proportion of n-grams generated by the model that also appear in the reference text.\n",
    "\n",
    "- **Recall:** The proportion of n-grams in the reference text that are correctly matched by the generated text.\n",
    "\n",
    "The final ROUGE score is typically expressed as an F-measure, which combines precision and recall into a single metric, providing a balanced evaluation.\n",
    "\n",
    "### Interpreting ROUGE Scores\n",
    "\n",
    "Higher ROUGE scores indicate better text quality. A score of 1.0 means perfect match between the generated and reference texts. ROUGE scores are usually reported as percentages.\n",
    "\n",
    "### Applications of ROUGE\n",
    "\n",
    "ROUGE is widely used in machine learning tasks involving text generation, such as:\n",
    "\n",
    "- **Machine Translation:** Evaluating the quality of translated text compared to the original source text.\n",
    "\n",
    "- **Text Summarization:** Assessing the effectiveness of generated summaries compared to the original full-length documents.\n",
    "\n",
    "- **Chatbots:** Evaluating the coherence and relevance of chatbot responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric=load_metric('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred  # Obtaining predictions and true labels\n",
    "    \n",
    "    # Decoding predictions\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    \n",
    "    # Obtaining the true labels tokens, while eliminating any possible masked token (i.e., label=-100)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # ROUGE expects a newline after each sentence\n",
    "    decoded_preds = ['\\n'.join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = ['\\n'.join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    \n",
    "    # Computing ROUGE score\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}  # Extracting some results\n",
    "    \n",
    "    # Adding mean-generated length\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result['gen_len'] = np.mean(prediction_lens)\n",
    "    \n",
    "    return {k: round(v, 4) for k, v in result.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainin the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataCollatorForSeq2Seq(tokenizer=BartTokenizerFast(name_or_path='facebook/bart-large-xsum', vocab_size=50265, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
      "}, model=BartForConditionalGeneration(\n",
      "  (model): BartModel(\n",
      "    (shared): Embedding(50264, 1024, padding_idx=1)\n",
      "    (encoder): BartEncoder(\n",
      "      (embed_tokens): Embedding(50264, 1024, padding_idx=1)\n",
      "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x BartEncoderLayer(\n",
      "          (self_attn): BartSdpaAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation_fn): GELUActivation()\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (decoder): BartDecoder(\n",
      "      (embed_tokens): Embedding(50264, 1024, padding_idx=1)\n",
      "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x BartDecoderLayer(\n",
      "          (self_attn): BartSdpaAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (activation_fn): GELUActivation()\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): BartSdpaAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1024, out_features=50264, bias=False)\n",
      "), padding=True, max_length=None, pad_to_multiple_of=None, label_pad_token_id=-100, return_tensors='pt')\n"
     ]
    }
   ],
   "source": [
    "# Data collator is used to format the training data. \n",
    "# It pads the input sequences to the maximum sequence length in the batch, and pads the target sequences to the maximum target length in the batch.\n",
    "\n",
    "data_collator= DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "print(data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = tokenized_datasets.remove_columns(dataset[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m num_train_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Show the training loss with every epoch\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m logging_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mtokenized_datasets\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m batch_size\n\u001b[0;32m      5\u001b[0m model_name \u001b[38;5;241m=\u001b[39m model_ckpt\n\u001b[0;32m      7\u001b[0m args \u001b[38;5;241m=\u001b[39m Seq2SeqTrainingArguments(\n\u001b[0;32m      8\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      9\u001b[0m     evaluation_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m     logging_steps\u001b[38;5;241m=\u001b[39mlogging_steps,\n\u001b[0;32m     18\u001b[0m     push_to_hub\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenized_datasets' is not defined"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "num_train_epochs = 8\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(tokenized_datasets[\"train\"]) // batch_size\n",
    "model_name = model_ckpt\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"{model_name}\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5.6e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    predict_with_generate=True,\n",
    "    logging_steps=logging_steps,\n",
    "    push_to_hub=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
