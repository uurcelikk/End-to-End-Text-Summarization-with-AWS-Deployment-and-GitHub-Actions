{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install transformers[sentencepiece] datasets sacrebleu rouge_score py7zr -q\n",
    "!pip install transformers==4.37.2\n",
    "!pip install datasets==2.17.0\n",
    "!pip install evaluate==0.4.1\n",
    "!pip install rouge-score==0.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import torch\n",
    "from datasets import Dataset, load_dataset, load_from_disk, load_metric\n",
    "from huggingface_hub import huggingface_hub\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, BartForConditionalGeneration, pipeline, set_seed\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')  # Downloading necessary NLTK data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "huggingface_hub.login(token=\"TOKEN_API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_ckpt = \"facebook/bart-large-xsum\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URL of the ZIP file\n",
    "url = 'https://github.com/entbappy/Branching-tutorial/raw/master/summarizer-data.zip'\n",
    "\n",
    "# Define the file paths\n",
    "zip_file_path = 'summarizer-data.zip'\n",
    "extracted_folder_path = 'summarizer-data'\n",
    "\n",
    "# Download the ZIP file\n",
    "response = requests.get(url)\n",
    "with open(zip_file_path, 'wb') as zip_file:\n",
    "    zip_file.write(response.content)\n",
    "\n",
    "# Extract the contents of the ZIP file\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extracted_folder_path)\n",
    "\n",
    "# Remove the ZIP file\n",
    "os.remove(zip_file_path)\n",
    "\n",
    "print('Downloaded and extracted the ZIP file successfully.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk(\"summarizer-data/samsum_dataset/\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate split lengths\n",
    "split_lengths = [len(dataset[split]) for split in dataset]\n",
    "\n",
    "\n",
    "print(\"Split lengths:\", split_lengths)\n",
    "print(\"Features:\", dataset['train'].column_names)\n",
    "print(\"\\nDialogue:\")\n",
    "print(dataset[\"test\"][1][\"dialogue\"])\n",
    "print(\"\\nSummary:\")\n",
    "print(dataset[\"test\"][1][\"summary\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(dataset[\"train\"])\n",
    "test_df = pd.DataFrame(dataset[\"test\"])\n",
    "validation_df = pd.DataFrame(dataset[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming train, test, and val are pandas DataFrames defined elsewhere in your code\n",
    "# Convert pandas DataFrames to Dataset objects\n",
    "train_ds = Dataset.from_pandas(train_df)\n",
    "test_ds = Dataset.from_pandas(test_df)\n",
    "val_ds = Dataset.from_pandas(validation_df)\n",
    "\n",
    "print(f\"Train Dataset:\\n{train_ds}\\n\\n\")\n",
    "print(f\"Test Dataset:\\n{test_ds}\\n\\n\")\n",
    "print(f\"Validation Dataset:\\n{val_ds}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_example(example):\n",
    "    \"\"\"\n",
    "    Preprocesses a single example (data point) for BART model input.\n",
    "\n",
    "    Args:\n",
    "        example: A dictionary containing 'dialogue' (list of text) and 'summary' (text).\n",
    "\n",
    "    Returns:\n",
    "        model_inputs: A dictionary of tokenized inputs and labels ready for BART. \n",
    "    \"\"\"\n",
    "\n",
    "    # Extract dialogues and prepare for tokenization\n",
    "    inputs = example['dialogue']\n",
    "\n",
    "    # Tokenize input dialogues for BART\n",
    "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
    "\n",
    "    # Tokenize target summaries\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(example['summary'], max_length=128, truncation=True)\n",
    "\n",
    "    # Include tokenized labels in model inputs\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "# Tokenize and preprocess datasets\n",
    "tokenized_train = train_ds.map(preprocess_example, batched=True, remove_columns=['id', 'dialogue', 'summary'])\n",
    "tokenized_test = test_ds.map(preprocess_example, batched=True, remove_columns=['id', 'dialogue', 'summary'])\n",
    "tokenized_val = val_ds.map(preprocess_example, batched=True, remove_columns=['id', 'dialogue', 'summary'])\n",
    "\n",
    "print(tokenized_train)\n",
    "print(tokenized_test)\n",
    "print(tokenized_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking samples\n",
    "sample = tokenized_train[0]\n",
    "print(\"Input IDs:\")\n",
    "print(sample['input_ids'])\n",
    "print(\"\\nAttention Mask:\")\n",
    "print(sample['attention_mask'])\n",
    "print(\"\\nLabels:\")\n",
    "print(sample['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BartForConditionalGeneration(\n",
      "  (model): BartModel(\n",
      "    (shared): Embedding(50264, 1024, padding_idx=1)\n",
      "    (encoder): BartEncoder(\n",
      "      (embed_tokens): Embedding(50264, 1024, padding_idx=1)\n",
      "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x BartEncoderLayer(\n",
      "          (self_attn): BartSdpaAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation_fn): GELUActivation()\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (decoder): BartDecoder(\n",
      "      (embed_tokens): Embedding(50264, 1024, padding_idx=1)\n",
      "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x BartDecoderLayer(\n",
      "          (self_attn): BartSdpaAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (activation_fn): GELUActivation()\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): BartSdpaAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1024, out_features=50264, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = BartForConditionalGeneration.from_pretrained(model_ckpt).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Evaluation Metric\n",
    "\n",
    "### What is ROUGE?\n",
    "\n",
    "ROUGE (ROUGE-L, ROUGE-N, ROUGE-W) is a widely used metric for evaluating the quality of machine-generated text, particularly in the context of text summarization and machine translation. It measures how well a generated text (candidate) matches a reference text (ground truth) by comparing their common n-grams (sequence of n consecutive characters).\n",
    "\n",
    "### How does ROUGE work?\n",
    "\n",
    "ROUGE calculates three main scores:\n",
    "\n",
    "- **ROUGE-L:** Focuses on matching long n-grams (typically n = 1 or 2). This is useful for evaluating fluency and coherence of the generated text.\n",
    "\n",
    "- **ROUGE-N:** Evaluates matches of n-grams of any length (typically n = 1, 2, 3, 4, 5, 6). This provides a more comprehensive assessment of the generated text's similarity to the reference.\n",
    "\n",
    "- **ROUGE-W:** Considers n-grams weighted by their frequency in the reference text. This emphasizes the importance of matching more common phrases.\n",
    "\n",
    "### ROUGE Scores\n",
    "\n",
    "Each ROUGE score is calculated as a combination of precision and recall:\n",
    "\n",
    "- **Precision:** The proportion of n-grams generated by the model that also appear in the reference text.\n",
    "\n",
    "- **Recall:** The proportion of n-grams in the reference text that are correctly matched by the generated text.\n",
    "\n",
    "The final ROUGE score is typically expressed as an F-measure, which combines precision and recall into a single metric, providing a balanced evaluation.\n",
    "\n",
    "### Interpreting ROUGE Scores\n",
    "\n",
    "Higher ROUGE scores indicate better text quality. A score of 1.0 means perfect match between the generated and reference texts. ROUGE scores are usually reported as percentages.\n",
    "\n",
    "### Applications of ROUGE\n",
    "\n",
    "ROUGE is widely used in machine learning tasks involving text generation, such as:\n",
    "\n",
    "- **Machine Translation:** Evaluating the quality of translated text compared to the original source text.\n",
    "\n",
    "- **Text Summarization:** Assessing the effectiveness of generated summaries compared to the original full-length documents.\n",
    "\n",
    "- **Chatbots:** Evaluating the coherence and relevance of chatbot responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\uurce\\AppData\\Local\\Temp\\ipykernel_18148\\4244457520.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric=load_metric('rouge')\n",
      "c:\\Users\\uurce\\anaconda3\\envs\\textS\\lib\\site-packages\\datasets\\load.py:753: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.0/metrics/rouge/rouge.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Downloading builder script: 5.65kB [00:00, 453kB/s]                    \n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric=load_metric('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\uurce\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred  # Obtaining predictions and true labels\n",
    "    \n",
    "    # Decoding predictions\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    \n",
    "    # Obtaining the true labels tokens, while eliminating any possible masked token (i.e., label=-100)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # ROUGE expects a newline after each sentence\n",
    "    decoded_preds = ['\\n'.join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = ['\\n'.join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    \n",
    "    # Computing ROUGE score\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}  # Extracting some results\n",
    "    \n",
    "    # Adding mean-generated length\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result['gen_len'] = np.mean(prediction_lens)\n",
    "    \n",
    "    return {k: round(v, 4) for k, v in result.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
