{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Classification\n",
      "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n",
      "Semi-supervised Sequence Learning\n",
      "Universal Language Model Fine-tuning for Text Classification\n",
      "Bag of Tricks for Efficient Text Classification\n",
      "RoBERTa: A Robustly Optimized BERT Pretraining Approach\n",
      "FastText.zip: Compressing text classification models\n",
      "Character-level Convolutional Networks for Text Classification\n",
      "Distributed Representations of Sentences and Documents\n",
      "Revisiting Semi-Supervised Learning with Graph Embeddings\n",
      "Very Deep Convolutional Networks for Text Classification\n"
     ]
    }
   ],
   "source": [
    "# Define the URL and fetch its content\n",
    "url = \"https://paperswithcode.com/task/text-classification\"\n",
    "html = requests.get(url)\n",
    "data = bs(html.content, \"html.parser\")\n",
    "\n",
    "titles = data.find_all('h1')\n",
    "\n",
    "\n",
    "for title in titles:\n",
    "    print(title.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listedeki öğe sayısı: 153\n"
     ]
    }
   ],
   "source": [
    "# Find all links ('a' tags) in the HTML content and create a list containing links that start with '/paper/'\n",
    "links = data.find_all('a')\n",
    "link_list = [link.get('href') for link in links if link.get('href') and link.get('href').startswith('/paper/')]\n",
    "\n",
    "\n",
    "print(\"Number of items in the list:\", len(link_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listedeki öğe sayısı: 46\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/paper/pushing-on-text-readability-assessment-a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/paper/nutcracker-at-wnut-2020-task-2-robustly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/paper/bag-of-tricks-for-efficient-text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/paper/anytime-active-learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/paper/the-re-label-method-for-data-centric-ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/paper/how-to-fine-tune-bert-for-text-classifi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/paper/multitask-semi-supervised-learning-for-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/paper/semi-supervised-sequence-learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>/paper/fasttextzip-compressing-text-classifica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>/paper/bert-of-all-trades-master-of-some</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>/paper/trans-blstm-transformer-with-bidirectional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>/paper/mwo2kg-and-echidna-constructing-and-exp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>/paper/breaking-free-transformer-models-task</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>/paper/text-classification-in-the-wild-a-large...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>/paper/a-comparative-study-of-feature-types-fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>/paper/very-deep-convolutional-networks-for-text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>/paper/linkbert-pretraining-language-models-with</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>/paper/selective-in-context-data-augmentation-for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>/paper/hierarchical-attentional-hybrid-neural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>/paper/roberta-a-robustly-optimized-bert-pretr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>/paper/protoformer-embedding-prototypes-for-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>/paper/vector-of-locally-aggregated-word-embed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>/paper/high-accuracy-rule-based-question</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>/paper/inferring-the-source-of-official-texts-can</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>/paper/distributed-representations-of-sentence...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>/paper/big-bird-transformers-for-longer-sequences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>/paper/revisiting-semi-supervised-learning-with</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>/paper/muld-the-multitask-long-document-benchmark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>/paper/when-does-pretraining-help-assessing-self</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>/paper/transformers-are-short-text-classifiers-a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>/paper/an-amharic-news-text-classification-dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>/paper/aggressionnet-generalised-multi-modal-deep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>/paper/bert-pre-training-of-deep-bidirectional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>/paper/mining-adverse-drug-reactions-from</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>/paper/a-comparison-of-svm-against-pre-trained</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>/paper/bertgcn-transductive-text-classificatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>/paper/universal-language-model-fine-tuning-fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>/paper/bert-based-ensembles-for-modeling-discl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>/paper/mteb-massive-text-embedding-benchmark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>/paper/xlnet-generalized-autoregressive-pretra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>/paper/supervised-and-semi-supervised-text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>/paper/character-level-convolutional-networks-for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>/paper/a-unified-neural-network-model-for-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>/paper/hierarchical-pre-training-for-sequence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>/paper/this-is-not-a-dataset-a-large-negation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>/paper/victor-a-dataset-for-brazilian-legal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Link\n",
       "0     /paper/pushing-on-text-readability-assessment-a\n",
       "1      /paper/nutcracker-at-wnut-2020-task-2-robustly\n",
       "2             /paper/bag-of-tricks-for-efficient-text\n",
       "3                      /paper/anytime-active-learning\n",
       "4   /paper/the-re-label-method-for-data-centric-ma...\n",
       "5   /paper/how-to-fine-tune-bert-for-text-classifi...\n",
       "6   /paper/multitask-semi-supervised-learning-for-...\n",
       "7            /paper/semi-supervised-sequence-learning\n",
       "8   /paper/fasttextzip-compressing-text-classifica...\n",
       "9            /paper/bert-of-all-trades-master-of-some\n",
       "10  /paper/trans-blstm-transformer-with-bidirectional\n",
       "11  /paper/mwo2kg-and-echidna-constructing-and-exp...\n",
       "12       /paper/breaking-free-transformer-models-task\n",
       "13  /paper/text-classification-in-the-wild-a-large...\n",
       "14  /paper/a-comparative-study-of-feature-types-fo...\n",
       "15   /paper/very-deep-convolutional-networks-for-text\n",
       "16   /paper/linkbert-pretraining-language-models-with\n",
       "17  /paper/selective-in-context-data-augmentation-for\n",
       "18      /paper/hierarchical-attentional-hybrid-neural\n",
       "19  /paper/roberta-a-robustly-optimized-bert-pretr...\n",
       "20      /paper/protoformer-embedding-prototypes-for-1\n",
       "21  /paper/vector-of-locally-aggregated-word-embed...\n",
       "22           /paper/high-accuracy-rule-based-question\n",
       "23  /paper/inferring-the-source-of-official-texts-can\n",
       "24  /paper/distributed-representations-of-sentence...\n",
       "25  /paper/big-bird-transformers-for-longer-sequences\n",
       "26    /paper/revisiting-semi-supervised-learning-with\n",
       "27  /paper/muld-the-multitask-long-document-benchmark\n",
       "28   /paper/when-does-pretraining-help-assessing-self\n",
       "29   /paper/transformers-are-short-text-classifiers-a\n",
       "30  /paper/an-amharic-news-text-classification-dat...\n",
       "31  /paper/aggressionnet-generalised-multi-modal-deep\n",
       "32     /paper/bert-pre-training-of-deep-bidirectional\n",
       "33          /paper/mining-adverse-drug-reactions-from\n",
       "34     /paper/a-comparison-of-svm-against-pre-trained\n",
       "35  /paper/bertgcn-transductive-text-classificatio...\n",
       "36  /paper/universal-language-model-fine-tuning-fo...\n",
       "37  /paper/bert-based-ensembles-for-modeling-discl...\n",
       "38       /paper/mteb-massive-text-embedding-benchmark\n",
       "39  /paper/xlnet-generalized-autoregressive-pretra...\n",
       "40         /paper/supervised-and-semi-supervised-text\n",
       "41  /paper/character-level-convolutional-networks-for\n",
       "42        /paper/a-unified-neural-network-model-for-1\n",
       "43      /paper/hierarchical-pre-training-for-sequence\n",
       "44      /paper/this-is-not-a-dataset-a-large-negation\n",
       "45        /paper/victor-a-dataset-for-brazilian-legal"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#clean the list\n",
    "new_link_list = [link.split('#code')[0] for link in link_list]\n",
    "\n",
    "# Remove duplicates from the list\n",
    "new_link_list = set(new_link_list)\n",
    "print(\"Number of items in the list::\", len(new_link_list))\n",
    "df = pd.DataFrame(new_link_list, columns=['Link'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstracts saved to abstracts.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "urls = list(new_link_list)\n",
    "abstracts = []\n",
    "\n",
    "# Extract abstracts from each paper\n",
    "for url in urls:\n",
    "    full_url = 'https://paperswithcode.com' + url\n",
    "    response = requests.get(full_url)\n",
    "    soup = bs(response.text, 'html.parser')\n",
    "   \n",
    "    abstract_div = soup.find('div', class_='paper-abstract')\n",
    "    abstract = abstract_div.p.text\n",
    "    abstracts.append(abstract)\n",
    "\n",
    "\n",
    "# Create a DataFrame and save the abstracts to a CSV file\n",
    "df = pd.DataFrame({'URL': urls, 'Abstract': abstracts})\n",
    "\n",
    "\n",
    "df.to_csv('abstracts.csv', index=False)\n",
    "\n",
    "print('Abstracts saved to abstracts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n            We report two essential improvements in readability assessment: 1. three novel features in advanced semantics and 2. the timely evidence that traditional ML models (e.g. Random Forest, using handcrafted features) can combine with transformers (e.g. RoBERTa) to augment model performance. First, we explore suitable transformers and traditional ML models. Then, we extract 255 handcrafted linguistic features using self-developed extraction software. Finally, we assemble those to create several hybrid models, achieving state-of-the-art (SOTA) accuracy on popular datasets in readability assessment. The use of handcrafted features help model performance on smaller datasets. Notably, our RoBERTA-RF-T1 hybrid achieves the near-perfect classification accuracy of 99%, a 20.3% increase from the previous SOTA.\\n        '"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('abstracts.csv', sep=',')\n",
    "df.iloc[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
